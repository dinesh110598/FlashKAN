# %%
import torch
from torch import nn

def unscaled_attention(query, key, value):
    return 0

class MultiHeadKANAttention(nn.Module):
    def __init__(self, embed_dim, num_heads, dropout):
        return 0
    