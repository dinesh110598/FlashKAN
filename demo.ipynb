{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashKAN: Grid size-independent computation of Kolmogorov Arnold networks using BSpline bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing a \"regular\" single layer KAN on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from layers import Regular_KAN, KANLinear, FlashKAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch = 200\n",
    "trainset = torchvision.datasets.MNIST(\"./Data\", train=True, download=True,\n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./Data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not concerned about performance (in terms of loss/accuracy) as much as training/inference speeds of our models. So we benchmark the training speed of 3 models for different grid sizes: $G = 10, 50, 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reg_kan(G):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        KANLinear(28*28, 10, G)\n",
    "    ).to(device)\n",
    "\n",
    "reg_kan1 = create_reg_kan(10)\n",
    "reg_kan2 = create_reg_kan(100)\n",
    "reg_kan3 = create_reg_kan(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = lambda out, labels: (torch.argmax(out,1) == labels).float().mean()\n",
    "\n",
    "opt1 = torch.optim.Adam(reg_kan1.parameters(), lr=0.001)\n",
    "opt2 = torch.optim.Adam(reg_kan2.parameters(), lr=0.001)\n",
    "opt3 = torch.optim.Adam(reg_kan3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a single epoch on the MNIST dataset\n",
    "def train_loop(net, opt, epochs=1):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        \n",
    "        for i, data in enumerate(trainloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc = metric(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_acc += acc.item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss*(batch/60_000), running_acc*(batch/60_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time(net, opt):\n",
    "    t0 = time()\n",
    "    _, _ = train_loop(net, opt)\n",
    "    t1 = time()\n",
    "    return t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 10  Time taken:  4.063426494598389\n",
      "G: 100  Time taken:  7.6076741218566895\n",
      "G: 500  Time taken:  28.56937551498413\n"
     ]
    }
   ],
   "source": [
    "print(\"G: 10 \", \"Time taken: \", train_time(reg_kan1, opt1))\n",
    "print(\"G: 100 \", \"Time taken: \", train_time(reg_kan2, opt2))\n",
    "print(\"G: 500 \", \"Time taken: \", train_time(reg_kan3, opt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken to perform a single epoch of training a single layer KAN on MNIST definitely increases with the grid size $G$.\n",
    "\n",
    "This implementation was taken from an existing work found in [efficient KAN](https://github.com/Blealtan/efficient-kan)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single layer FlashKAN on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from layers.FlashKAN import FlashKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we use a different implementation of KAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flash_kan(G):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        FlashKAN(28*28, 10, G)\n",
    "    ).to(device)\n",
    "\n",
    "flash_kan1 = create_flash_kan(10)\n",
    "flash_kan2 = create_flash_kan(100)\n",
    "flash_kan3 = create_flash_kan(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = lambda out, labels: (torch.argmax(out,1) == labels).float().mean()\n",
    "\n",
    "opt1 = torch.optim.Adam(flash_kan1.parameters(), lr=0.001)\n",
    "opt2 = torch.optim.Adam(flash_kan2.parameters(), lr=0.001)\n",
    "opt3 = torch.optim.Adam(flash_kan3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 10  Time taken:  7.005167007446289\n",
      "G: 100  Time taken:  7.101221323013306\n",
      "G: 500  Time taken:  7.40746808052063\n"
     ]
    }
   ],
   "source": [
    "print(\"G: 10 \", \"Time taken: \", train_time(flash_kan1, opt1))\n",
    "print(\"G: 100 \", \"Time taken: \", train_time(flash_kan2, opt2))\n",
    "print(\"G: 500 \", \"Time taken: \", train_time(flash_kan3, opt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time remains roughly the same for larger grid sizes! The benefits are especially apparent for $G>100$. However, the size of model parameters and the memory complexity are roughly the same. \n",
    "\n",
    "This implementation exploits the limited support (parts of the domain where it has non-zero value) of the BSpline basis functions on the given grid and slices only parts of the weight array that support the given input data point before multiplication. Slicing the weight array also necessitates a custom gradient defined by subclassing `torch.autograd.Function`\n",
    "\n",
    "See implementation in `FlashKAN.py` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not just fast but actually works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a single layer FlashKAN on MNIST for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash_kan = create_flash_kan(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly tweaked training loop logging every epoch\n",
    "def train_loop2(net, opt, epochs=1):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        for i, data in enumerate(trainloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc = metric(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_acc += acc.item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}  Loss: {running_loss / i:.3f}', \n",
    "            f'Accuracy: {running_acc / i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.998 Accuracy: 0.6991\n",
      "Epoch: 2  Loss: 0.490 Accuracy: 0.8539\n",
      "Epoch: 3  Loss: 0.382 Accuracy: 0.8850\n",
      "Epoch: 4  Loss: 0.325 Accuracy: 0.9053\n",
      "Epoch: 5  Loss: 0.268 Accuracy: 0.9218\n",
      "Epoch: 6  Loss: 0.240 Accuracy: 0.9307\n",
      "Epoch: 7  Loss: 0.224 Accuracy: 0.9353\n",
      "Epoch: 8  Loss: 0.225 Accuracy: 0.9323\n",
      "Epoch: 9  Loss: 0.187 Accuracy: 0.9469\n",
      "Epoch: 10  Loss: 0.170 Accuracy: 0.9507\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(flash_kan.parameters(), lr=0.001)\n",
    "\n",
    "train_loop2(flash_kan, opt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(net):\n",
    "    running_loss, running_acc = 0., 0.\n",
    "    for i, data in enumerate(testloader, 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = metric(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_acc += acc.item()\n",
    "        running_loss += loss.item()\n",
    "    print(f'loss: {running_loss / i:.3f}', \n",
    "            f'accuracy: {running_acc / i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.322 accuracy: 0.9061\n"
     ]
    }
   ],
   "source": [
    "test_fn(flash_kan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric for both train and test datasets are above 90%, which means we get pretty much the same performance (as other KAN implementations) for better speeds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
