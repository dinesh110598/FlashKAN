{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FlashKAN: Grid size-independent computation of Kolmogorov Arnold networks using BSpline bases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Timing a \"regular\" single layer KAN on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from time import time\n",
    "from Regular_KAN import Regular_KAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select GPU if available\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda:0\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loading the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.ToTensor()\n",
    "\n",
    "batch = 200\n",
    "trainset = torchvision.datasets.MNIST(\"./Data\", train=True, download=True,\n",
    "                                      transform=transform)\n",
    "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch,\n",
    "                                          shuffle=True)\n",
    "\n",
    "testset = torchvision.datasets.MNIST(root='./Data', train=False,\n",
    "                                       download=True, transform=transform)\n",
    "testloader = torch.utils.data.DataLoader(testset, batch_size=batch,\n",
    "                                         shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're not concerned about performance (in terms of loss/accuracy) as much as training/inference speeds of our models. So we benchmark the training speed of 3 models for different grid sizes: $G = 10, 50, 100$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_reg_kan(G):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        Regular_KAN(28*28, 10, G)\n",
    "    ).to(device)\n",
    "\n",
    "reg_kan1 = create_reg_kan(10)\n",
    "reg_kan2 = create_reg_kan(50)\n",
    "reg_kan3 = create_reg_kan(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = lambda out, labels: (torch.argmax(out,1) == labels).float().mean()\n",
    "\n",
    "opt1 = torch.optim.Adam(reg_kan1.parameters(), lr=0.001)\n",
    "opt2 = torch.optim.Adam(reg_kan2.parameters(), lr=0.001)\n",
    "opt3 = torch.optim.Adam(reg_kan3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trains a single epoch on the MNIST dataset\n",
    "def train_loop(net, opt, epochs=1):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        for i, data in enumerate(trainloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc = metric(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_acc += acc.item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "    return running_loss*(batch/60_000), running_acc*(batch/60_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_time(net, opt):\n",
    "    t0 = time()\n",
    "    _, _ = train_loop(net, opt)\n",
    "    t1 = time()\n",
    "    return t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 10  Time taken:  38.593632221221924\n",
      "G: 50  Time taken:  133.68002247810364\n",
      "G: 100  Time taken:  337.3856027126312\n"
     ]
    }
   ],
   "source": [
    "print(\"G: 10 \", \"Time taken: \", train_time(reg_kan1, opt1))\n",
    "print(\"G: 50 \", \"Time taken: \", train_time(reg_kan2, opt2))\n",
    "print(\"G: 100 \", \"Time taken: \", train_time(reg_kan3, opt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time taken to perform a single epoch of training a single layer KAN on MNIST definitely increases with the grid size $G$.\n",
    "\n",
    "Implementation details can be found in `Regular_KAN.py`. This is comparable in performance to implementations found in other sources like [efficient KAN](https://github.com/Blealtan/efficient-kan), especially wrt varying grid sizes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a single layer FlashKAN on the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FlashKAN import FlashKAN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, we use a different implementation of KAN:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_flash_kan(G):\n",
    "    return nn.Sequential(\n",
    "        nn.Flatten(),\n",
    "        FlashKAN(28*28, 10, G)\n",
    "    ).to(device)\n",
    "\n",
    "flash_kan1 = create_flash_kan(10)\n",
    "flash_kan2 = create_flash_kan(50)\n",
    "flash_kan3 = create_flash_kan(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "metric = lambda out, labels: (torch.argmax(out,1) == labels).float().mean()\n",
    "\n",
    "opt1 = torch.optim.Adam(flash_kan1.parameters(), lr=0.001)\n",
    "opt2 = torch.optim.Adam(flash_kan2.parameters(), lr=0.001)\n",
    "opt3 = torch.optim.Adam(flash_kan3.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "G: 10  Time taken:  12.132895708084106\n",
      "G: 50  Time taken:  13.694677352905273\n",
      "G: 100  Time taken:  15.494819164276123\n"
     ]
    }
   ],
   "source": [
    "print(\"G: 10 \", \"Time taken: \", train_time(flash_kan1, opt1))\n",
    "print(\"G: 50 \", \"Time taken: \", train_time(flash_kan2, opt2))\n",
    "print(\"G: 100 \", \"Time taken: \", train_time(flash_kan3, opt3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training time remains roughly the same for larger grid sizes! However, the size of model parameters remains the same as the regular GANs. \n",
    "\n",
    "This implementation exploits the limited support (parts of the domain where it has non-zero value) of the BSpline basis functions on the given grid and slices only parts of the weight array that support the given input data point before multiplication. Slicing the weight array also necessitates a custom gradient defined by subclassing `torch.autograd.Function`\n",
    "\n",
    "See implementation in `KAN_Linear.py` for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Not just fast but actually works!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's train a single layer FlashKAN on MNIST for 10 epochs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "flash_kan = create_flash_kan(100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Slightly tweaked training loop logging every epoch\n",
    "def train_loop2(net, opt, epochs=1):\n",
    "    for epoch in range(epochs):  # loop over the dataset multiple times\n",
    "\n",
    "        running_loss = 0.\n",
    "        running_acc = 0.\n",
    "        for i, data in enumerate(trainloader, 1):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            opt.zero_grad()\n",
    "            \n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            with torch.no_grad():\n",
    "                acc = metric(outputs, labels)\n",
    "\n",
    "            # print statistics\n",
    "            running_acc += acc.item()\n",
    "            running_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch: {epoch+1}  Loss: {running_loss / i:.3f}', \n",
    "            f'Accuracy: {running_acc / i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  Loss: 0.962 Accuracy: 0.7042\n",
      "Epoch: 2  Loss: 0.461 Accuracy: 0.8626\n",
      "Epoch: 3  Loss: 0.380 Accuracy: 0.8859\n",
      "Epoch: 4  Loss: 0.320 Accuracy: 0.9051\n",
      "Epoch: 5  Loss: 0.267 Accuracy: 0.9214\n",
      "Epoch: 6  Loss: 0.244 Accuracy: 0.9288\n",
      "Epoch: 7  Loss: 0.220 Accuracy: 0.9360\n",
      "Epoch: 8  Loss: 0.194 Accuracy: 0.9449\n",
      "Epoch: 9  Loss: 0.168 Accuracy: 0.9544\n",
      "Epoch: 10  Loss: 0.161 Accuracy: 0.9553\n"
     ]
    }
   ],
   "source": [
    "opt = torch.optim.Adam(flash_kan.parameters(), lr=0.001)\n",
    "\n",
    "train_loop2(flash_kan, opt, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_fn(net):\n",
    "    running_loss, running_acc = 0., 0.\n",
    "    for i, data in enumerate(testloader, 1):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            # forward + backward + optimize\n",
    "            outputs = net(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            acc = metric(outputs, labels)\n",
    "\n",
    "        # print statistics\n",
    "        running_acc += acc.item()\n",
    "        running_loss += loss.item()\n",
    "    print(f'loss: {running_loss / i:.3f}', \n",
    "            f'accuracy: {running_acc / i:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss: 0.325 accuracy: 0.9045\n"
     ]
    }
   ],
   "source": [
    "test_fn(flash_kan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy metric for both train and test datasets are above 90%, which means we get pretty much the same performance (as other KAN implementations) for better speeds!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
